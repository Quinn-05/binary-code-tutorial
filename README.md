# Understanding Binary Code Basics

**Author**: Quinn Howald  
**Purpose**: This tutorial introduces binary code, the foundation of digital computing, explaining its structure and how to convert between binary and decimal numbers. It’s aimed at beginners who want to understand how computers process data.  
**Target Audience**: This tutorial is designed for high school or early college students with little to no prior knowledge of digital systems.

---

## Contents
1. [Introduction to Binary Code](./README.md#introduction-to-binary-code)
2. [Understanding Binary Digits (Bits)](./README.md#understanding-binary-digits-bits)
3. [Converting Between Binary and Decimal](./binary_to_decimal.md)
4. [How Binary Code is Used in Computers](./binary_in_computing.md)
5. [Conclusion](./README.md#conclusion)

---

## Introduction to Binary Code
Binary code is the language of computers. It consists of only two symbols: `0` and `1`. All data in computers is ultimately represented as sequences of these two digits, called binary digits, or "bits."

## Understanding Binary Digits (Bits)
Each bit can represent two states: `0` or `1`. Together, multiple bits can represent larger numbers or complex data. 

## Conclusion
Now you should have a basic understanding of how binary code works and why it’s important in the world of computing.
